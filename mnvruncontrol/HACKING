mnvruncontrol -- the MINERvA run control
========================================
  Original author:  J. Wolcott  (jwolcott@fnal.gov)
  Last revision:    v4r4p1  (April 2010)
  Last update:      April 13, 2010
========================================

This document contains a few notes on the internal operations of the run control
software so as to give someone intending to work on it a general idea of what's
going on (and maybe how to avoid catastrophically destroying it).


Most of the code is intended to be relatively well-documented inline, so only
a few things need amplification here:
  * thread-safe programming and the run control;
  * the DataAcquisitionManager event model; and
  * the SocketThread message handling system.

(1) THREAD-SAFE PROGRAMMING
  (a) Method calls; events.
    By its nature the run control has to manage multiple threads of operation
    simultaneously.  It therefore cannot operate in a linear fashion; it needs
    EVENTS to signal from one thread to another.  wxPython has a robust event
    system in place, and the run control (hereafter "RC") takes advantage of it;
    all communication between threaded processes (except in the dispatchers) is
    done using the wxPython events created in backend/Events.py.
    
    It is very important to know that this is not just elegant modeling; it is
    absolutely essential to the stability of the processes.  Because Python
    feels so "free-form," it is very tempting to just directly call the
    member methods of one threaded object from the context of another.  DON'T.
    It invariably leads to what I call "hiccups", or sometimes even larger-scale
    freezing.  Pass an *event* to the client object instead, using the wx.Post()
    method, and have the event handler for the event in the client object call
    the method you want.
    
    For example:
    
    The "Pre-", "Post-", and "SkipToNextSubrun" methods of the main RunControl
    class need to be called around the beginning of, end of, or in between
    subruns, respectively.  Now, the sequencing of events is handled by the
    DataAcquisitionManager, so it seems simple enough for it to call
    (for instance)
       self.main_window.PreSubrun()
    from the StartNextSubrun() method.  But notice that this isn't what it does;
    instead, it posts a SubrunStartingEvent to the main_window object:
       wx.PostEvent(self.main_window, Events.SubrunStartingEvent(...))
    (line 370 or so; I have of course elided the parameters inside the 
    SubrunStartingEvent so as not to clutter the text here).  This is the
    procedure that should always be followed to maintain the stability of the
    run.
    
  (b) Synchronization primitives.
    Other synchronization methods are used as necessary.  The message handler
    for the DataAcquisitionManager (the method HandleSocketMessage), for
    example, uses a threading.Lock object to ensure that multiple messages are
    not processed concurrently.  It would be wise to have at least a decent
    introduction to synchronization primitives (mutual exclusion locks,
    semaphores, etc.) under your belt before attempting to modify the run
    control (particularly the DataAcquisitionManager or the objects found in
    backend/Threads.py) too much.  The 'threading' Python module provides a
    nice interface to pretty much any synchronization task you'll need to do, so
    be sure to consult the Python documentation (http://docs.python.org/)
    before going to town on it.  Carefully think about the potential
    consequences of multiple threads executing the same method, and use
    synchronization tools appropriately.
    
    Inline comments are sprinkled throughout the code to illustrate the logic
    I've used.  (I don't claim to be a great operating system programmer, so if
    you are, and you find design flaws in my threading model, please feel free
    to correct them!)

(2) THE EVENT MODEL OF DATAACQUISITIONMANAGER

    The DataAcquisitionManager object is the heart and soul of the run control
    system.  
    
    The nature of the underlying processes it's coordinating means that it needs
    to execute its startup sequence in a somewhat non-deterministic fashion.  To
    accomodate this, the DataAcquisitionManager class's methods for subrun
    startup follow a somewhat opaque (but, I hope, logical!) structure.
    Hopefully the discussion below will clarify it enough to make further work
    on it possible as necessary.  Comments within the codebase itself are also
    intended to help illustrate what's happening.
    
    There are effectively three non-deterministic components to subrun startup,
    and all are handled in more or less the same way: first, if the hardware
    configuration on the readout nodes needs to be changed, this can take a
    substantial (and somewhat unpredictable) length of time; next, the sequence
    may involve a check of PMT high voltages, which might need to consult the
    user (who can spend an indefinite amount of time deciding whether it's safe
    to continue the run); and finally, the Event Transfer ("ET") and DAQ processes
    that are started as subprocesses can take a variable amount of time to set
    up.  The procedure for dealing with each of them is described in turn below
    (though note that the first and second items above are dealt with together
    in part (a)).
    
    Following the text descriptions is an attempt to represent this in a flow
    chart form.
    
  (a) Subrun startup tasks -- handling PMT high voltage checks and slow control
      configuration.
    The fact that the startup sequence needs to be able to be interrupted and
    resumed is accommodated by introducing a sort of "task list" of jobs that
    need to be accomplished (in a certain order) before a run can start.  You
    can find the tasks that are on the slate in the "SubrunStartTasks" attribute
    of the DataAcquisitionManager (currently lines 65 and following).  The list
    there consists of references to the METHODS (yep, that's right, you can
    store references to methods as variables in Python) that need to be executed
    --in the appropriate order--as well as descriptions of what they're doing
    that will be displayed on the front panel.  When StartDataAcquisition is
    called, each of these tasks is assigned one final attribute: "completed".
    You might be able to guess what this is for. :)
    
    Each time the StartNextSubrun() method is called, it goes through the list
    in order.  If a method is not yet "completed", it is called.  Each task that
    is executed is first marked "completed."  Each is allowed to return one of
    three values:
       True,  if it completed successfully;
       False, if it failed (and the run should be stopped);
    or None,  if it initiated some non-determinate task that needs to be waited
              on.
    So, if True was returned, the next task will be executed immediately; if
    False, the subrun will be stopped immediately; and, if None, StartNextSubrun
    will immediately exit and wait to be called again.  The next time it's
    called, it will resume with the next method that is not yet "completed".
    
    Note that the StartNextSubrun() method should only be called as the event
    handler for the ReadyforNextSubrunEvent (the one exception being when it
    is called directly from StartDataAcquisition), so each task that needs to
    run for an indeterminate period should post a ReadyForNextSubrunEvent to
    the DataAcquisitionManager when it's done.
    
    When the hardware configuration is being written to the hardware, it is the
    HandleSocketMessage that issues such an event (after receiving an appropriate
    message from the readout nodes via the SocketThread); in the case of the PMT
    high-voltage check, it's issued instead by the HVConfirmationFrame
    (in frontend/Frames.py).  [See part (3) below for more information on the
    SocketThread/messaging system.]

  (b) Subprocess startup.
    The indeterminate nature of the subprocess startup time is handled in a
    similar fashion to that of the other startup tasks.  There's a list of
    DAQStartTasks, akin to the SubrunStartTasks above, currently at line 70ff in
    the code, with the same properties.  They are executed in much the same way.
    
    The only new wrinkle is that since the subprocesses are separate programs,
    they don't have access to the memory space of the DataAcquisionManager and
    so they can't use wx.Post() to send events when they're ready.  Instead,
    we rely on the interprocess signalling mechanisms built into the UNIX core.
    Each of the subprocesses that requires a wait issues a SIGUSR1 signal back
    to the main RunControl process using the kill() function of the standard
    UNIX library when it is ready for the next process to start up.  (You can
    read more about kill() and other UNIX C library functions at
    http://www.gnu.org/software/libc/.)  The DataAcquisitionManager,
    meanwhile, sets itself up to handle this signal using the "signal" method
    of the "signal" module (who'd've guessed?...) in StartDataAcquisition, and
    the method that's called as the signal handler (StartNextThread) creates
    a new subprocess when the signal is received.
    
    When all of the processes have started, data acquisition should be underway,
    so the last process to start posts a notice to this effect to the log file,
    then quits.  Further activity will be initiated by the HandleSocketMessage
    method after receiving notification from the SocketThread (which by this
    point is listening for "done" messages from the readout nodes).
    
 
 Left: DataAcquisitionManager                                         :    Right: graphics/front end
              methods                                                 :                 methods
                                                                      :
                     |--------------------|                           :
                     |StartDataAcquisition|                           :
                     |--------------------|                           :
                              |                                       :
                              v                                       :
        |-->------------>-----|                                       :
        |                     v                                       :
        |             |---------------|               SubrunStartingEvent               |---------|
        |         |->-|StartNextSubrun|------------>--------------------->--------------|PreSubrun|
        |  Ready  |   |---------------|                               :                 |---------|
        |    For  |           |                                       :
        | ForNext |           v                                       :
        |  Subrun |   |---------------|                               :
        |  Event  |-<-|SubrunStartTask|                               :
        |             |---------------|                               :
        ^                     |                                       :
        |                     | when all                              :
  Ready |                     | tasks                                 :
   For  |                     | completed                             :
   Next |                     v                                       :
 Subrun |             |---------------|           |-------------|     :
  Event |             |StartDAQThreads|--->--->---|ThreadStarter|     :
        |             |---------------|           |-------------|     :
   or   |                     |    ^                     |            :
        |         when all    |    |                     v            :                   |------|
  Skip  |           threads   |    |  sends SIGUSR1 |---------|      NewDataEvent         |DAQ/ET|
   To   |           started   |    |------<---<-----|DAQThread|------->--------->---------|window|
  Next  ^                     |        when ready   |---------|       :                   |------|
 Subrun |                     |                                       :
  Event |                     v                                       :
        |             |---------------|                               :
        |             |   [waiting]   |               UpdateProgressEvent             |---------------|
        |             | (SocketThread |------------->------------------->-------------|UpdateRunStatus|
        |             |   listening)  |                               :               |---------------|
        |             |---------------|                               :
        |                     |                                       :
        |                     | when messages                         :
        ^                     | received from                         :
        |                     | all readout nodes                     :
        |                     v                                       :
        |  if more      |---------|                   SubrunOverEvent :                 |----------|
        |-----<------<--|EndSubrun|----------------->------------------>----------------|PostSubrun|
             subruns    |---------|                                   :                 |----------|
                              |                                       :
                      Subrun  | no more subruns                       :
                       Over   | or "Stop" pressed                     :
                      Event   |                                       :
                              v                                       :
                    |-------------------|                             :
                    |StopDataAcquisition|                             :
                    |-------------------|                             :
                    
(3) The SocketThread message handling system.
  The SocketThread (backend/Threads.py) is a thread designed to listen in the
  background on behalf of the DataAcquisitionManager for messages coming from
  remote nodes.  Because it's listening for messages from various sources
  simultaneously, it uses a somewhat different paradigm from the dispatchers
  (which expect useful communication from a single source at a time and discard
  other communcations).
  
  The basic idea is as follows: every message intended for the dispatcher
  should conform to a particular format specifying who it's from, who it's to,
  and the content of the message.  (See the code for this format.)  The "from"
  field is determined by the identity assigned to the dispatcher on that node
  when a lock is requested (see backend/RemoteNode.py and backend/Dispatcher.py).
  The "to" field will be the identifier the RemoteNode object was assigned when
  it was created.  The message content is arbitrary.
  
  When expecting a message, a receiving function needs to "subscribe" to the
  SocketThread (using its Subscribe() method, surprisingly enough) giving the
  name and ID of the node it's expecting a message from (each RemoteNode
  instance has these as attributes) and the message content.  A subscription
  must also provide an object reference known as a "callback", which is the
  object to which an Event should be delivered when a corresponding message is
  received.  When a message conforming to the format is received, the
  SocketThread examines its list of current subscriptions, and if one of them
  matches the message, it posts a SocketReceiptEvent containing attributes
  corresponding to the message properties to the callback object.
  
  If the program is actively waiting on some message, it can be helpful to get
  some kind of indication that things are still going.  The SocketThread will
  post UpdateProgressEvents containing a certain message every 0.25s to the
  callback if the "waiting" and "notice" parameters are passed to the Subscribe
  method.
  
  Most of this should (at this point) be transparent to the programmer.  See
  DataAcquisitionManager.ReadoutNodeHWConfig() for an example of how to book
  subscriptions, and look at DataAcquisitionManager.HandleSocketMessage() to
  see how SocketReceiptEvents might be handled.
  
  
